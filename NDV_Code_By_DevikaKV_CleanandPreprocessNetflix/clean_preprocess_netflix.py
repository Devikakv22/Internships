# -*- coding: utf-8 -*-
"""clean_preprocess_netflix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M1lXbDxHpdt47hd-cOjXVXx1fnVrrVOn
"""

from google.colab import files
uploaded = files.upload()

# Netflix Dataset Cleaning & Preprocessing

# Step 1: Imports
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# Step 2: Load Dataset
df = pd.read_csv('netflix_titles.csv')  # Ensure this file is in the working directory

# Step 3: Initial Inspection
print(df.head())
print(df.info())
print(df.describe())
print("\nMissing values per column:\n", df.isnull().sum())
print("\nNumber of duplicate rows:", df.duplicated().sum())

# Step 4: Handle Missing Values
df['country'].fillna('Unknown', inplace=True)
df.dropna(subset=['director'], inplace=True)
df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')

# Step 5: Remove Duplicates
df.drop_duplicates(inplace=True)

# Step 6: Data Type Fixes
print("\nData types after conversion:\n", df.dtypes)

# Step 7: NumPy Transformation - Description Length
df['desc_len'] = np.vectorize(len)(df['description'].astype(str))

# Step 8: Pandas Filtering, Sorting, Grouping
movies_df = df[df['type'] == 'Movie']
sorted_df = df.sort_values('release_year', ascending=False)
type_counts = df.groupby('type').size()
print("\nType counts:\n", type_counts)

# Step 9: Visualizations
plt.figure(figsize=(10, 5))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

plt.figure(figsize=(10, 5))
sns.heatmap(df.select_dtypes(include=np.number).corr(), annot=True)
plt.title('Correlation Matrix')
plt.show()

# Step 10: Label Encoding
df['type_encoded'] = LabelEncoder().fit_transform(df['type'])

# Step 11: Save Cleaned Data
df.to_csv('netflix_cleaned.csv', index=False)
print("\nCleaned data saved as 'netflix_cleaned.csv'")